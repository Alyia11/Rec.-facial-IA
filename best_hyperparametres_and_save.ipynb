{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'optimisation du modèle VGG16 pour votre ensemble de données, voici quelques paramètres qui pourraient être intéressants à optimiser :\n",
    "\n",
    "Taux d'apprentissage (learning rate) : Le taux d'apprentissage contrôle la taille des pas effectués lors de la mise à jour des poids du modèle. Un taux d'apprentissage trop élevé peut entraîner une convergence lente ou une instabilité, tandis qu'un taux d'apprentissage trop faible peut entraîner une convergence lente ou un risque de rester coincé dans un minimum local. Vous pouvez essayer différentes valeurs de taux d'apprentissage pour trouver celui qui fonctionne le mieux pour votre ensemble de données.\n",
    "\n",
    "Nombre d'époques (epochs) : Le nombre d'époques correspond au nombre de fois où l'ensemble de données complet est passé par le modèle lors de l'entraînement. Trop peu d'époques peuvent entraîner un sous-apprentissage, tandis que trop d'époques peuvent entraîner un surapprentissage. Vous pouvez effectuer une validation croisée avec différents nombres d'époques pour trouver le bon équilibre.\n",
    "\n",
    "Taille du batch (batch size) : La taille du batch détermine le nombre d'échantillons utilisés pour mettre à jour les poids du modèle à chaque itération. Une taille de batch trop petite peut entraîner une convergence lente, tandis qu'une taille de batch trop grande peut nécessiter plus de mémoire et ralentir l'entraînement. Vous pouvez essayer différentes tailles de batch pour voir leur impact sur les performances du modèle.\n",
    "\n",
    "Régularisation (regularization) : La régularisation est utilisée pour prévenir le surapprentissage en ajoutant une pénalité aux poids du modèle. Vous pouvez expérimenter avec différentes techniques de régularisation, telles que la pénalité L1 (Lasso) ou L2 (Ridge), ainsi qu'ajuster les paramètres de régularisation pour trouver le bon équilibre entre ajustement et généralisation.\n",
    "\n",
    "Architecture du réseau (network architecture) : Bien que le modèle VGG16 ait une architecture prédéfinie, vous pouvez explorer la possibilité d'ajouter ou de modifier certaines couches pour mieux s'adapter à votre ensemble de données. Par exemple, vous pouvez ajouter des couches de régularisation supplémentaires, ajuster le nombre de filtres dans les couches de convolution, ou modifier la taille des couches entièrement connectées.\n",
    "\n",
    "Ces paramètres peuvent être optimisés à l'aide de techniques d'optimisation telles que la recherche par grille (grid search) ou l'optimisation bayésienne. Vous pouvez également utiliser des bibliothèques telles que scikit-learn ou Optuna pour faciliter ce processus d'optimisation des hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "import os\n",
    "import sklearn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "import torch.utils.data as data_utils\n",
    "from random import randint\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ResNet_9</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [\n",
    "              nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "              nn.BatchNorm2d(out_channels),\n",
    "              nn.ReLU(inplace=True)\n",
    "    ]\n",
    "\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self,  num_classes=7, in_channels = 1, lr = 0.01,  dropout = 0.5, num_hidden = 4096, model_name = \"ResNet9\"):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.num_hidden = num_hidden\n",
    "        self.model_name = model_name\n",
    "        self.conv1 = conv_block(in_channels, 16, pool=False) # 16 x 48 x 48\n",
    "        self.conv2 = conv_block(16, 32, pool=True) # 32 x 24 x 24\n",
    "        self.res1 = nn.Sequential( #  32 x 24 x 24\n",
    "            conv_block(32, 32, pool=False), \n",
    "            conv_block(32, 32, pool=False)\n",
    "        )\n",
    "\n",
    "        self.conv3 = conv_block(32, 64, pool=True) # 64 x 12 x 12\n",
    "        self.conv4 = conv_block(64, 128, pool=True) # 128 x 6 x 6\n",
    "\n",
    "        self.res2 = nn.Sequential( # 128 x 6 x 6\n",
    "             conv_block(128, 128), \n",
    "             conv_block(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2), # 128 x 3 x 3\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*3*3, self.num_hidden), #512\n",
    "            nn.Linear(self.num_hidden, num_classes) # 7\n",
    "        )\n",
    "        self.network = nn.Sequential(\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.res1,\n",
    "            self.conv3,\n",
    "            self.conv4,\n",
    "            self.res2,\n",
    "            self.classifier,\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> New Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model48(nn.Module):\n",
    "    def __init__(self,  num_classes=7, in_channels = 1, lr = 0.01,  dropout = 0.5, num_hidden = 4096, model_name = \"Model48\"):\n",
    "        super(Model48, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.num_hidden = num_hidden\n",
    "        self.model_name = model_name\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 16 x 24 x 24\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 12 x 12\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 6 x 6\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(128*6*6, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout-0.1),\n",
    "            nn.Linear(512, self.num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Last Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self,  num_classes=7, in_channels = 1, lr = 0.01,  dropout = 0.5, num_hidden = 4096, model_name = \"EmotionRecognitionModel\"):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(64 * 10 * 10, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, 7)\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.num_hidden = num_hidden\n",
    "        self.model_name = model_name\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Create Folder </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder():\n",
    "    date = datetime.datetime.now().strftime(\"%m-%d\")\n",
    "    \n",
    "    folder_name = \"modele\"\n",
    "    if not os.path.exists(folder_name): os.makedirs(folder_name)\n",
    "\n",
    "    folder_name = f\"modele/hyperparameters_search_{date}\"\n",
    "    if not os.path.exists(folder_name): os.makedirs(folder_name)\n",
    "    \n",
    "    folder_name = \"graphs\"\n",
    "    if not os.path.exists(folder_name): os.makedirs(folder_name)\n",
    "\n",
    "    folder_name = f\"graphs/hyperparameters_search_{date}\"\n",
    "    if not os.path.exists(folder_name): os.makedirs(folder_name)\n",
    "    \n",
    "    return date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Save model </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(date, train_loss_history, train_acc_history, val_loss_history, val_acc_history, model, optimizer, epochs) :\n",
    "    nom_fichier = f\"_tr-acc{(train_acc_history[-1]*100):.1f}_val-acc{(val_acc_history[-1]*100):.1f}\"\n",
    "    file_path = f'modele/cross_val_{date}/{nom_fichier}.pth'\n",
    "    \n",
    "    torch.save({\n",
    "        'epochs': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'lr': model.lr,\n",
    "        'batch_size': model.batch_size,\n",
    "        'dropout': model.dropout,\n",
    "        'num_hidden': model.num_hidden,\n",
    "        'num_classes': model.num_classes,\n",
    "        'in_channels': model.in_channels,\n",
    "        'train_loss': train_loss_history,\n",
    "        'train_accuracy': train_acc_history,\n",
    "        'val_loss': val_loss_history,\n",
    "        'val_accuracy': val_acc_history,\n",
    "    }, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(train_loss_history, train_acc_history, val_loss_history, val_acc_history, model_name, date):\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(train_loss_history, label='Train Loss')\n",
    "        plt.plot(val_loss_history, label='Test Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f\"Graphe de perte pour {model_name}\")\t\n",
    "        plt.savefig(f'graphs/hyperparameters_search_{date}/loss_{model_name}_acc_{val_acc_history[-1]}.png')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(train_acc_history, label='Train Accuracy')\n",
    "        plt.plot(val_acc_history, label='Test Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title(f\"Graphe d'accuracy pour {model_name}\")\n",
    "        plt.savefig(f'graphs/hyperparameters_search_{date}/acc_{model_name}_acc_{val_acc_history[-1]}.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load Data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    len_of_task = randint(3, 20)  # take some random length of time\n",
    "    \n",
    "    data = pd.read_csv('fer2013.csv')\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        faces.append(face.astype('float32'))\n",
    "\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "\n",
    "    # Normalize the pixels\n",
    "    faces /= 255.0\n",
    "\n",
    "    # Emotion labels\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(faces, dtype=torch.float32)\n",
    "    y = torch.tensor(emotions, dtype=torch.long)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data (X,y, batch_size = 32) :\n",
    "    #use train test split to split our data into 80% training 20% testing\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Créer des objets DataLoader pour les ensembles d'entraînement et de validation\n",
    "    train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = data_utils.TensorDataset(X_val, y_val)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train Data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, test_loader, date, optimizer, criterion, epochs=10):\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Traitement en cours\", bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        \n",
    "        for inputs, targets in train_loader :\n",
    "            inputs = inputs.permute(0, 3, 1, 2)  # Changez l'ordre des dimensions pour correspondre à l'entrée du modèle\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs[:targets.size(0)])\n",
    "            loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += targets.size(0) \n",
    "            correct_train += predicted.eq(torch.max(targets, 1)[1]).sum().item()\n",
    "\n",
    "        # Calcul des métriques de performance pour l'ensemble d'entraînement\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "\n",
    "        # Évaluation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.permute(0, 3, 1, 2)\n",
    "                outputs = model(inputs[:targets.size(0)])  \n",
    "                loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_test += targets.size(0) \n",
    "                correct_test += predicted.eq(torch.max(targets, 1)[1]).sum().item()\n",
    "\n",
    "        # Calcul des métriques de performance pour l'ensemble de test\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = correct_test / len(test_loader.dataset)\n",
    "\n",
    "        # Affichage des métriques de performance\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}')\n",
    "\n",
    "        # Stocker les métriques de performance pour chaque itération\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    save_model(date, train_losses, train_accuracies, test_losses, test_accuracies, model, optimizer, epochs)\n",
    "    plot_loss_acc( train_losses, train_accuracies, test_losses, test_accuracies, model.model_name, date)\n",
    "        \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
    "\n",
    "def train_model(train_loader, test_loader, X_train, X_val, y_train, y_val, date, model_name = \"Model48\", lr=0.001, batch_size=32, dropout = 0.2, num_hidden = 4096,  epochs=10, save = False):\n",
    "    if model_name == \"Model48\" :\n",
    "        model = Model48(num_classes=7, in_channels=1 ,lr=lr, dropout=dropout, num_hidden=num_hidden)\n",
    "    elif model_name == \"EmotionRecognitionModel\" :\n",
    "        model = EmotionRecognitionModel(num_classes=7, in_channels=1 ,lr=lr, dropout=dropout, num_hidden=num_hidden)\n",
    "    elif model_name == \"ResNet\" :\n",
    "        model = ResNet9(num_classes=7, in_channels=1 ,lr=lr, dropout=dropout, num_hidden=num_hidden)\n",
    "    else :\n",
    "        raise NameError(\"Model name not found\")\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.lr)\n",
    "\n",
    "    if save == True :\n",
    "        # run function fit\n",
    "        train_losses, train_accuracies, test_losses, test_accuracies = fit(model, train_loader, test_loader, date, optimizer, criterion, epochs=epochs)\n",
    "        return model, train_losses, train_accuracies, test_losses, test_accuracies\n",
    "    else :\n",
    "        return fit(model, train_loader, test_loader, date, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y, date):\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1)\n",
    "    epochs = trial.suggest_int('epochs', 5, 20)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    num_hidden = trial.suggest_categorical('num_hidden', [512, 1024, 2048, 4096])\n",
    "    model_name = trial.suggest_categorical('model_name', [\"Model48\", \"EmotionRecognitionModel\", \"ResNet\"])\n",
    "\n",
    "    print(\"hyperparameters: {}\".format(trial.params))\n",
    "    \n",
    "    train_loader, test_loader, X_train, X_val, y_train, y_val = split_data(X, y, batch_size=batch_size)\n",
    "    \n",
    "    # Entraînement du modele avec la fonction train_model\n",
    "    train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(train_loader, test_loader, X_train, X_val, y_train, y_val, date, model_name = model_name, lr=lr, batch_size=batch_size, dropout = dropout, num_hidden = num_hidden,  epochs=epochs)\n",
    "    return val_acc_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-14 20:35:32,026]\u001b[0m A new study created in memory with name: no-name-63dc95ca-395f-48d1-a036-5c827398f5bf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters: {'lr': 0.025761839231960088, 'epochs': 11, 'batch_size': 16, 'dropout': 0.22001646877195702, 'num_hidden': 2048, 'model_name': 'ResNet'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement en cours: 100%|██████████| 1795/1795 [05:23<00:00,  5.54it/s]\n",
      "Traitement en cours:   9%|▉         | 1/11 [05:42<57:09, 342.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.8309, Train Acc: 0.2131, Test Loss: 0.1142, Test Acc: 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement en cours: 100%|██████████| 1795/1795 [06:19<00:00,  4.73it/s]\n",
      "Traitement en cours:  18%|█▊        | 2/11 [12:22<56:25, 376.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.1159, Train Acc: 0.2294, Test Loss: 0.1136, Test Acc: 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X,y = load_data()\n",
    "date = create_folder()\n",
    "\n",
    "func = lambda trial: objective(trial, X, y, date)\n",
    "\n",
    "study = optuna.create_study(direction = \"maximize\")\n",
    "study.optimize(func, n_trials=10)\n",
    "\n",
    "trial = study.best_trial\n",
    "#print accuracy and best parameters\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tracer la courbe de perte\n",
    "# plt.plot(val_loss_history_bp, label='HP Val loss')\n",
    "# plt.plot(val_loss_history, label='Val Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.show()\n",
    "\n",
    "# # Tracer la courbe d'accuracy\n",
    "# plt.plot(val_acc_history_bp, label='HP Val Accuracy')\n",
    "# plt.plot(val_acc_history, label='Val Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
