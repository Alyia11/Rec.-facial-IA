{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'optimisation du modèle VGG16 pour votre ensemble de données, voici quelques paramètres qui pourraient être intéressants à optimiser :\n",
    "\n",
    "Taux d'apprentissage (learning rate) : Le taux d'apprentissage contrôle la taille des pas effectués lors de la mise à jour des poids du modèle. Un taux d'apprentissage trop élevé peut entraîner une convergence lente ou une instabilité, tandis qu'un taux d'apprentissage trop faible peut entraîner une convergence lente ou un risque de rester coincé dans un minimum local. Vous pouvez essayer différentes valeurs de taux d'apprentissage pour trouver celui qui fonctionne le mieux pour votre ensemble de données.\n",
    "\n",
    "Nombre d'époques (epochs) : Le nombre d'époques correspond au nombre de fois où l'ensemble de données complet est passé par le modèle lors de l'entraînement. Trop peu d'époques peuvent entraîner un sous-apprentissage, tandis que trop d'époques peuvent entraîner un surapprentissage. Vous pouvez effectuer une validation croisée avec différents nombres d'époques pour trouver le bon équilibre.\n",
    "\n",
    "Taille du batch (batch size) : La taille du batch détermine le nombre d'échantillons utilisés pour mettre à jour les poids du modèle à chaque itération. Une taille de batch trop petite peut entraîner une convergence lente, tandis qu'une taille de batch trop grande peut nécessiter plus de mémoire et ralentir l'entraînement. Vous pouvez essayer différentes tailles de batch pour voir leur impact sur les performances du modèle.\n",
    "\n",
    "Régularisation (regularization) : La régularisation est utilisée pour prévenir le surapprentissage en ajoutant une pénalité aux poids du modèle. Vous pouvez expérimenter avec différentes techniques de régularisation, telles que la pénalité L1 (Lasso) ou L2 (Ridge), ainsi qu'ajuster les paramètres de régularisation pour trouver le bon équilibre entre ajustement et généralisation.\n",
    "\n",
    "Architecture du réseau (network architecture) : Bien que le modèle VGG16 ait une architecture prédéfinie, vous pouvez explorer la possibilité d'ajouter ou de modifier certaines couches pour mieux s'adapter à votre ensemble de données. Par exemple, vous pouvez ajouter des couches de régularisation supplémentaires, ajuster le nombre de filtres dans les couches de convolution, ou modifier la taille des couches entièrement connectées.\n",
    "\n",
    "Ces paramètres peuvent être optimisés à l'aide de techniques d'optimisation telles que la recherche par grille (grid search) ou l'optimisation bayésienne. Vous pouvez également utiliser des bibliothèques telles que scikit-learn ou Optuna pour faciliter ce processus d'optimisation des hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "import os\n",
    "import sklearn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "import torch.utils.data as data_utils\n",
    "from rich import progress\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVGG16(nn.Module):\n",
    "    def __init__(self, lr = 0.01,  num_classes=7, in_channels = 1):\n",
    "        super(MyVGG16, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    len_of_task = randint(3, 20)  # take some random length of time\n",
    "    \n",
    "    data = pd.read_csv('fer2013.csv')\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        faces.append(face.astype('float32'))\n",
    "\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "\n",
    "    # Normalize the pixels\n",
    "    faces /= 255.0\n",
    "\n",
    "    # Emotion labels\n",
    "    emotions = pd.get_dummies(data['emotion']).values\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.tensor(faces, dtype=torch.float32)\n",
    "    y = torch.tensor(emotions, dtype=torch.long)\n",
    "    return X,y\n",
    "\n",
    "X,y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data (X,y, batch_size = 32) :\n",
    "    #use train test split to split our data into 80% training 20% testing\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Créer des objets DataLoader pour les ensembles d'entraînement et de validation\n",
    "    train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = data_utils.TensorDataset(X_val, y_val)\n",
    "    test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, X_train, X_val, y_train, y_val\n",
    "\n",
    "#call the function above to split our data\n",
    "train_loader, test_loader, X_train, X_val, y_train, y_val = split_data(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement en cours:   0%|          | 3/898 [00:13<1:08:59,  4.63s/it]\n",
      "Traitement en cours:   0%|          | 0/10 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[39mreturn\u001b[39;00m fit(model, train_loader, test_loader, optimizer, criterion, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[39m#call the function above to train our model\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m train_loss_history, train_acc_history, val_loss_history, val_acc_history \u001b[39m=\u001b[39m train_model(train_loader, test_loader, X_train, X_val, y_train, y_val)\n",
      "Cell \u001b[1;32mIn[27], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_loader, test_loader, X_train, X_val, y_train, y_val, lr, epochs, save)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m model, fit(model, train_loader, test_loader, optimizer, criterion, epochs\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m fit(model, train_loader, test_loader, optimizer, criterion, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[27], line 19\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, test_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# Changez l'ordre des dimensions pour correspondre à l'entrée du modèle\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs[:targets\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)])\n\u001b[0;32m     20\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, torch\u001b[39m.\u001b[39mmax(targets, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m])\n\u001b[0;32m     21\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[24], line 56\u001b[0m, in \u001b[0;36mMyVGG16.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fit(model, train_loader, test_loader, optimizer, criterion, epochs=10):\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Traitement en cours\", bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "        # Entraînement\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=\"Entraînement en cours\", bar_format=\"{l_bar}{bar:10}{r_bar}\"):\n",
    "            inputs = inputs.permute(0, 3, 1, 2)  # Changez l'ordre des dimensions pour correspondre à l'entrée du modèle\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs[:targets.size(0)])\n",
    "            loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_train += targets.size(0) \n",
    "            correct_train += predicted.eq(torch.max(targets, 1)[1]).sum().item()\n",
    "\n",
    "        # Calcul des métriques de performance pour l'ensemble d'entraînement\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / len(train_loader.dataset)\n",
    "\n",
    "        # Évaluation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0 \n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.permute(0, 3, 1, 2)\n",
    "                outputs = model(inputs[:targets.size(0)])  \n",
    "                loss = criterion(outputs, torch.max(targets, 1)[1])\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total_test += targets.size(0) \n",
    "                correct_test += predicted.eq(torch.max(targets, 1)[1]).sum().item()\n",
    "\n",
    "        # Calcul des métriques de performance pour l'ensemble de test\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_accuracy = correct_test / len(test_loader.dataset)\n",
    "\n",
    "        # Affichage des métriques de performance\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}')\n",
    "\n",
    "        # Stocker les métriques de performance pour chaque itération\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
    "\n",
    "def train_model(train_loader, test_loader, X_train, X_val, y_train, y_val, lr=0.001, epochs=10, save = False):\n",
    "\n",
    "    model = MyVGG16(lr = lr,  num_classes=7, in_channels = 1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=model.lr)\n",
    "\n",
    "    if save == True :\n",
    "        return model, fit(model, train_loader, test_loader, optimizer, criterion, epochs=epochs)\n",
    "    else :\n",
    "        return fit(model, train_loader, test_loader, optimizer, criterion, epochs=10)\n",
    "\n",
    "#call the function above to train our model\n",
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(train_loader, test_loader, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(train_loss_history, train_acc_history, val_loss_history, val_acc_history):\n",
    "\n",
    "    # Tracer la courbe de perte\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Tracer la courbe d'accuracy\n",
    "    plt.plot(train_acc_history, label='Train Accuracy')\n",
    "    plt.plot(val_acc_history, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss_acc(train_loss_history, train_acc_history, val_loss_history, val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X, y):\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1)\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    train_loader, split_data(X,y, batch_size=batch_size)\n",
    "    # Entraînement du modele avec la fonction train_model\n",
    "    train_loss_history, train_acc_history, val_loss_history, val_acc_history = train_model(train_loader, test_loader, X_train, X_val, y_train, y_val, lr=lr, epochs=epochs)\n",
    "    return val_acc_history[-1]\n",
    "    \n",
    "\n",
    "func = lambda trial: objective(trial, X_train, X_val, y_train, y_val)\n",
    "\n",
    "study = optuna.create_study(direction = \"maximize\")\n",
    "study.optimize(func, n_trials=10)\n",
    "\n",
    "trial = study.best_trial\n",
    "#print accuracy and best parameters\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best parameters\n",
    "lr = trial.params['lr']\n",
    "epochs = trial.params['epochs']\n",
    "batch_size = trial.params['batch_size']\n",
    "\n",
    "#train the model with the best parameters\n",
    "train_loader, split_data(X,y, batch_size=batch_size)\n",
    "model, train_loss_history_bp, train_acc_history_bp, val_loss_history_bp, val_acc_history_bp = train_model(train_loader, test_loader, X_train, X_val, y_train, y_val, lr=lr, epochs=epochs, save= True)\n",
    "\n",
    "#save the model with pytorch\n",
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la courbe de perte\n",
    "plt.plot(val_loss_history_bp, label='HP Val loss')\n",
    "plt.plot(val_loss_history, label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Tracer la courbe d'accuracy\n",
    "plt.plot(val_acc_history_bp, label='HP Val Accuracy')\n",
    "plt.plot(val_acc_history, label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
